/*

Purpose:
    1) To showcase user how to use the Kafka transport in Siddhi.
    2) Explain how to map from one data type to another (e.g, JSON to XML')

How to setup:
    1) The following steps must be executed to enable WSO2 SP to receive and publish events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding.
        a) Download the Kafka broker from here: https://www.apache.org/dyn/closer.cgi?path=/kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz
        b) Convert and copy the Kafka client jars from the {KafkaHome}/libs directory to the {WSO2SPHome}/libs directory as follows.
              i) Create a directory named {Source} in a preferred location in your machine and copy the following JARs to it from the {KafkaHome}/libs directory.
                 * kafka_2.11-0.9.0.1.jar
                 * kafka-clients-0.9.0.1.jar
                 * metrics-core-2.2.0.jar
                 * scala-library-2.11.7.jar
                 * scala-parser-combinators_2.11-1.0.4.jar
                 * zkclient-0.7.jar
                 * zookeeper-3.4.6.jar
             ii) Create another directory named {Destination} in a preferred location in your machine.
            iii) To convert all the Kafka jars you copied into the {Source} directory, issue the following command.
                 * For Windows: {WSO2SPHome}/bin/jartobundle.bat <{Source} Directory Path> <{Destination} Directory Path>
                 * For Linux: sh {WSO2SPHome}/bin/jartobundle.sh <{Source} Directory Path> <{Destination} Directory Path>
             iv) Add the OSGI converted kafka libs from {Destination} directory to {WSO2SPHome}/lib
              v) Add the original Kafka libs from {Source} to {WSO2SPHome}/samples/sample-clients/lib
             vi) Navigate to {KafkaHome} and start zookeeper node using "sh bin/zookeeper-server-start.sh config/zookeeper.properties" command
            vii) Navigate to {KafkaHome} and start Kafka server node using "sh bin/kafka-server-start.sh config/server.properties" command
           viii) Start the server: sh editor.sh
    2) Save this sample as HelloKafka.siddhi
    3) Start the siddhi app by clicking on 'Run' (Click 'Run' on menu bar -> 'Run'). If you edit this application while it's running, stop the app (Click 'Run' on menu bar -> 'Stop') -> Save app -> Start app
    4) Navigate to {WSO2SPHome}/samples/sample-clients/kafka-consumer and run "ant" command without arguments
    5) Navigate to {WSO2SPHome}/samples/sample-clients/kafka-producer and run "ant -Dbroker=localhost:9092 -DtopicName=kafka_topic -DsampleNo=1007 -DfileName=kafka_sample". This command would publish the events in kafka_sample file to the source Kafka Topic (named 'kafka_topic')
    6) Note that the events in file 'kafka_sample' are in JSON format. Hence the source configuration's map type is given as JSON
    7) See the output events received by sink Kafka Topic (named 'kafka_result_topic') being logged on the 'kafka-consumer' console. Note how the events have been converted from JSON to XML type. This conversion happens due to sink configuration's map type being XML.
    8) Stop this siddhi app, once you are done with the execution

*/

@App:name("HelloKafka")

@App:description('HelloKafka siddhi application helps you to understand how to use Kafka transport in siddhi to consume events from a Kafka Topic and publish events to a different Kafka Topic. It further explores how data can be converted from one type to another (e.g, JSON to XML')

@source(type='kafka',
        topic.list='kafka_topic',
        partition.no.list='0',
        threading.option='single.thread',
        group.id="group",
        bootstrap.servers='localhost:9092',
        @map(type='json'))
define stream inputStream(symbol string, price float, volume long);

@sink(type='kafka',
      topic='kafka_result_topic',
      bootstrap.servers='localhost:9092',
      partition.no='0',
      @map(type='xml'))
define stream outputStream(symbol string, price float, totalVolume long);

from inputStream#window.length(5)
select symbol, price, volume as totalVolume
insert into outputStream;


